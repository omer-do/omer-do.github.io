<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes Control Plane (K8s series - 5) | od-stack</title><meta name=keywords content="kubernetes,containers,infrastructure"><meta name=description content="How the Kubernetes control plane works — etcd, the API server, controllers, and the scheduler."><meta name=author content="Omer Dolev"><link rel=canonical href=https://omer-do.github.io/posts/2020-12-15-kubernetes-control-plane/><link crossorigin=anonymous href=/assets/css/stylesheet.61ac92b0e7992b16abbfb57d04b87f47ab6eb38d284d11fe00f817d29ee90bba.css integrity="sha256-YaySsOeZKxarv7V9BLh/R6tus40oTRH+APgX0p7pC7o=" rel="preload stylesheet" as=style><link rel=icon href=https://omer-do.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://omer-do.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://omer-do.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://omer-do.github.io/apple-touch-icon.png><link rel=mask-icon href=https://omer-do.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://omer-do.github.io/posts/2020-12-15-kubernetes-control-plane/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://omer-do.github.io/posts/2020-12-15-kubernetes-control-plane/"><meta property="og:site_name" content="od-stack"><meta property="og:title" content="Kubernetes Control Plane (K8s series - 5)"><meta property="og:description" content="How the Kubernetes control plane works — etcd, the API server, controllers, and the scheduler."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-15T09:50:00+08:00"><meta property="article:modified_time" content="2020-12-15T09:50:00+08:00"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Containers"><meta property="article:tag" content="Infrastructure"><meta property="og:image" content="https://omer-do.github.io/img/kubernetes-control-plane-4.png"><meta property="og:see_also" content="https://omer-do.github.io/posts/2020-12-16-how-it-all-fits-together/"><meta property="og:see_also" content="https://omer-do.github.io/posts/2020-12-14-low-and-high-level-runtimes/"><meta property="og:see_also" content="https://omer-do.github.io/posts/2020-12-11-container-runtimes/"><meta property="og:see_also" content="https://omer-do.github.io/posts/2020-12-11-containers/"><meta property="og:see_also" content="https://omer-do.github.io/posts/2020-12-11-hello-kubernetes/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://omer-do.github.io/img/kubernetes-control-plane-4.png"><meta name=twitter:title content="Kubernetes Control Plane (K8s series - 5)"><meta name=twitter:description content="How the Kubernetes control plane works — etcd, the API server, controllers, and the scheduler."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://omer-do.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Kubernetes Control Plane (K8s series - 5)","item":"https://omer-do.github.io/posts/2020-12-15-kubernetes-control-plane/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes Control Plane (K8s series - 5)","name":"Kubernetes Control Plane (K8s series - 5)","description":"How the Kubernetes control plane works — etcd, the API server, controllers, and the scheduler.","keywords":["kubernetes","containers","infrastructure"],"articleBody":"Overview In K8s, as in many other products the architecture is a manager/master and worker nodes architecture.\nI’m not going to cover everything in just one post, so for now let’s focus on the control plane.\nIf you needed to manage something like this, orchestrating containers, in a very large scale (hundreds to thousands of containers), how would you do it?\nThat’s a very big question. But, let’s break it down to simpler objectives or tasks.\nLet’s start with the basis for this application. The first thing we might ask ourselves is how would this application know what to do? When to do it? And what to do it to?\nAt the lowest level, a good way to start, is to somehow know the objective (the desired state), and find our way of getting to this desired state. Basically we need a place to hold our objective, components will be able to check what the desired state is and then decide what actions need to be taken to get us there.\nOK then! This application would have a part that does the work, and it needs a place for data.\nWe will need a “single source of truth” (if we have more than one place where the desired state resides and somehow these different places hold different states, we will have chaos). This single source of truth will have to be accessible to every component. Also, to handle such scale, it needs to be distributed because we might need many worker nodes, doing loads of actions, also the control plane parts are going to perform many administrative actions as well.\nThere is another point here. Let’s say we got our database, if many different components will have to perform operations on it, then we will be compelled not to only have a logic in each component that connects to the database, we will also have to make sure that the operations are valid (so we don’t have any corruption).\nDoing these validations and support large operations scale is not an easy task. So, we should also have some kind of a gateway, via which components can access the data. It’s easier to validate data, manage, and control. In addition, It’s also good if we have a standard way of interactions between components, so we should maybe consider having RESTful components.\nETCD \u0026 API That’s where ETCD and the API server of K8s come into play. ETCD is a RESTful hierarchical distributed key-value datastore that can handle large scales (highly available), supports watching (watching for changes of entries) and secure connections.\nFun Fact: ETCD uses the raft census algorithm to create a quorum for leader election. The leader is the member through which writes are commited to ensure consistency between member’s data. I will have a post about raft, as it’s a cool algorithm that many tools use.\nThe API server is the gateway to the ETCD, all operations and changes to the desired state are going through the API server whose job is not only to perform them but to also, to validate, ensure the standardization of data and structures and check authentication and authorization to perform such actions. It’s also designed to be scaled up horizontally, to support high traffic.\nHurray! We have our truth source and the gateway to it :)\nWhat we need now is the tools that will actually make the actions.\nWhat actions are we talking about though? Well…\nChecking the desired state and the actual state and bringing the actual state closer to the desired one.\nAlso, We need to know what we are going to manage.\nThe managees are the K8s resources presented below:\nController and Scheduler The doers of the control plane: the controller manager, and the scheduler.\nLike Controlling Stuff? The controller manager is responsible for the controllers, the parts that really do stuff. So what’s a controller?\nA very good explanation is in the K8s official Docs:\nIn applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.\nIn this excerpt (^^) some controllers are mentioned, and though it seems that controllers are exclusively handling a single resource, they are not. Controller are more “actions” oriented. Maybe actions sometimes involve a single object or resource type, but sometimes it’s not like that. For example take a look at the Tokens Controller and it’s Github if you’d like.\nSo, a controller checks if there are ways that the desired and actual state are different, then it does what’s called a reconcile which is “bringing the current state to the desired state”. These “reconcile” actions are done differently and independently by different controllers.\nThe 2 main components of a controller are an Informer/SharedInformer and a WorkQueue.\nThe Informer is a structure that holds a few things.\na local cache where the controller can watch a list of resources and their changes in state (whenever an object is deleted, modified or created) resource event handler that configures an AddFunc (for when an object is added), an UpdateFunc (for when an object is updated) and a DeleteFunc(for when an object is deleted) resync period which sets an interval for when the controller should trigger the UpdateFunc on the items remaining in the cache. This provides a kind of configuration to periodically verify the current state and make it like the desired state. It’s extremely useful in the case where the controller may have missed updates or prior actions failed. SharedInformer is just like a regular informer but it’s shared (as it’s name implies) among controllers. To share caches that watch resources eliminates duplication of cached resources, saves connections and improves overall costs for the ‘watch’ action.\nMost controllers are using the SharedInformer. So they share the cached list of resources they need to watch, but the actions they want to perform for each change is different. So you can’t have the same logic running in all the controllers that look at deployments for example. That’s why each controller has a WorkQueue, whenever a resource changes the event handler pushes a key to the WorkQueue. The controller reads off this queue and handles the reconciliation.\nAbout Scheding The scheduler is in charge of assigning pods to nodes. Sounds EZ right? not that simple… (also read Julia Evans post about the scheduler and the github)\nThe scheduler is a kind of a controller. The desired state is that every pod has a node assigned to it, it looks for pods without nodes, and tries to make actions towards the desired state (assigning them to nodes).\nWe could image the control loop like so\nwhile True: pods = get_all_pods() for pod in pods: if pod.node == nil: assignNode(pod) It’s not exactly like that though…\nAfter a little bit of digging, what actually happens inside the scheduler is:\nevery pod that needs scheduling get added to a queue (the only resources scheduled are pods) when a new pod is created it also gets added to the queue the scheduler takes pods of the queue, and schedules them But what happens if a pods fails? If there is an error when scheduling the pod, it calls an error handler, and the error handler puts it back in the queue.\nOk wait… isn’t the previous Python implementation better? for performance reasons, no…\nFor more info about scheduling optimizations read at the following: CoreOS - Improving Kubernetes Scheduler Performance\nFun Fact: The scheduler uses an Informer too (like controllers usually do)\nAnother Fun Fact: The scheduler unlike controllers, doesn’t resync. and that’s something that was decided by the maintainers:\n@brendandburns - what is it supposed to fix? I’m really against having such small resync periods, because it will significantly affect performance.\nand\nI agree with @wojtek-t . If resync ever fixes a problem, it means there is an underlying correctness bug that we are hiding. I do not think resync is the right solution.\nTo sum up, the overall architecture looks roughly like so:\n","wordCount":"1384","inLanguage":"en","image":"https://omer-do.github.io/img/kubernetes-control-plane-4.png","datePublished":"2020-12-15T09:50:00+08:00","dateModified":"2020-12-15T09:50:00+08:00","author":{"@type":"Person","name":"Omer Dolev"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://omer-do.github.io/posts/2020-12-15-kubernetes-control-plane/"},"publisher":{"@type":"Organization","name":"od-stack","logo":{"@type":"ImageObject","url":"https://omer-do.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://omer-do.github.io/ accesskey=h title="od-stack (Alt + H)">od-stack</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://omer-do.github.io/ title=Home><span>Home</span></a></li><li><a href=https://omer-do.github.io/about/ title=About><span>About</span></a></li><li><a href=https://omer-do.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://omer-do.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://omer-do.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://omer-do.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Kubernetes Control Plane (K8s series - 5)</h1><div class=post-meta><span title='2020-12-15 09:50:00 +0800 +0800'>December 15, 2020</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>Omer Dolev</span></div></header><figure class=entry-cover><img loading=eager src=https://omer-do.github.io/img/kubernetes-control-plane-4.png alt="Kubernetes Control Plane"></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a></li><li><a href=#etcd--api aria-label="ETCD & API">ETCD & API</a></li><li><a href=#controller-and-scheduler aria-label="Controller and Scheduler">Controller and Scheduler</a><ul><li><a href=#like-controlling-stuff aria-label="Like Controlling Stuff?">Like Controlling Stuff?</a></li><li><a href=#about-scheding aria-label="About Scheding">About Scheding</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>In K8s, as in many other products the architecture is a manager/master and worker nodes architecture.</p><p>I&rsquo;m not going to cover everything in just one post, so for now let&rsquo;s focus on the control plane.<br>If you needed to manage something like this, orchestrating containers, in a very large scale (hundreds to thousands of containers), how would you do it?</p><p>That&rsquo;s a very big question. But, let&rsquo;s break it down to simpler objectives or tasks.</p><p>Let&rsquo;s start with the basis for this application. The first thing we might ask ourselves is how would this application know what to do? When to do it? And what to do it to?<br>At the lowest level, a good way to start, is to somehow know the objective (the desired state), and find our way of getting to this desired state. Basically we need a place to hold our objective,
components will be able to check what the desired state is and then decide what actions need to be taken to get us there.</p><p>OK then! This application would have a part that does the work, and it needs a place for data.<br>We will need a &ldquo;single source of truth&rdquo; (if we have more than one place where the desired state resides and somehow these different places hold different states, we will have chaos).
This single source of truth will have to be accessible to every component.
Also, to handle such scale, it needs to be distributed because we might need many worker nodes, doing loads of actions, also the control plane parts are going to perform many administrative actions as well.</p><p>There is another point here. Let&rsquo;s say we got our database, if many different components will have to perform operations on it, then we will be compelled not to only have a logic in each component that connects to the database, we will also have to make sure that the operations are valid (so we don&rsquo;t have any corruption).<br>Doing these validations and support large operations scale is not an easy task.
So, we should also have some kind of a gateway, via which components can access the data. It&rsquo;s easier to validate data, manage, and control.
In addition, It&rsquo;s also good if we have a standard way of interactions between components, so we should maybe consider having RESTful components.</p><h2 id=etcd--api>ETCD & API<a hidden class=anchor aria-hidden=true href=#etcd--api>#</a></h2><p><img alt=Image2 loading=lazy src=/img/kubernetes-control-plane-2.png></p><p>That&rsquo;s where ETCD and the API server of K8s come into play. ETCD is a RESTful hierarchical distributed key-value datastore that can handle large scales (highly available), supports watching (watching for changes of entries)
and secure connections.</p><p><strong><em>Fun Fact</em></strong>: ETCD uses the raft census algorithm to create a quorum for leader election. The leader is the member through which writes are commited to ensure consistency between member&rsquo;s data.
I will have a post about raft, as it&rsquo;s a cool algorithm that many tools use.</p><p>The API server is the gateway to the ETCD, all operations and changes to the desired state are going through the API server whose job is not only to perform them but to also, to validate, ensure the standardization
of data and structures and check authentication and authorization to perform such actions. It&rsquo;s also designed to be scaled up horizontally, to support high traffic.</p><p>Hurray! We have our truth source and the gateway to it :)</p><p>What we need now is the tools that will actually make the actions.<br>What actions are we talking about though? Well&mldr;<br>Checking the desired state and the actual state and bringing the actual state closer to the desired one.</p><p>Also, We need to know what we are going to manage.<br>The managees are the K8s resources presented below:</p><p><img alt=Image3 loading=lazy src=/img/kubernetes-control-plane-3.png></p><h2 id=controller-and-scheduler>Controller and Scheduler<a hidden class=anchor aria-hidden=true href=#controller-and-scheduler>#</a></h2><p>The doers of the control plane: the controller manager, and the scheduler.</p><h3 id=like-controlling-stuff>Like Controlling Stuff?<a hidden class=anchor aria-hidden=true href=#like-controlling-stuff>#</a></h3><p>The controller manager is responsible for the controllers, the parts that really do stuff. So what&rsquo;s a controller?</p><p>A very good explanation is in the <a href=https://kubernetes.io/docs/admin/kube-controller-manager/>K8s official Docs</a>:</p><blockquote><p>In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API server and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.</p></blockquote><p>In this excerpt (^^) some controllers are mentioned, and though it seems that controllers are exclusively handling a single resource, they are not. Controller are more &ldquo;actions&rdquo; oriented. Maybe actions sometimes involve a single object or resource type, but sometimes it&rsquo;s not like that. For example take a look at the <a href=https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#token-controller>Tokens Controller</a> and it&rsquo;s <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/serviceaccount/tokens_controller.go>Github</a> if you&rsquo;d like.</p><p>So, a controller checks if there are ways that the desired and actual state are different, then it does what&rsquo;s called a <em>reconcile</em> which is &ldquo;bringing the current state to the desired state&rdquo;.
These &ldquo;reconcile&rdquo; actions are done differently and independently by different controllers.</p><p>The 2 main components of a controller are an Informer/SharedInformer and a WorkQueue.<br>The Informer is a structure that holds a few things.</p><ul><li>a local <strong>cache</strong> where the controller can watch a list of resources and their changes in state (whenever an object is deleted, modified or created)</li><li><strong>resource event handler</strong> that configures an AddFunc (for when an object is added), an UpdateFunc (for when an object is updated) and a DeleteFunc(for when an object is deleted)</li><li><strong>resync period</strong> which sets an interval for when the controller should trigger the UpdateFunc on the items remaining in the cache. This provides a kind of configuration to periodically verify the current state and make it like the desired state. It&rsquo;s extremely useful in the case where the controller may have missed updates or prior actions failed.</li></ul><p>SharedInformer is just like a regular informer but it&rsquo;s shared (as it&rsquo;s name implies) among controllers. To share caches that watch resources eliminates duplication of cached resources, saves connections and improves overall costs for the &lsquo;watch&rsquo; action.</p><p>Most controllers are using the SharedInformer. So they share the cached list of resources they need to watch, but the actions they want to perform for each change is different. So you can&rsquo;t have the same logic running in all the controllers that look at <em>deployments</em> for example. That&rsquo;s why each controller has a WorkQueue, whenever a resource changes the event handler pushes a key to the WorkQueue. The controller reads off this queue and handles the reconciliation.</p><h3 id=about-scheding>About Scheding<a hidden class=anchor aria-hidden=true href=#about-scheding>#</a></h3><p>The scheduler is in charge of assigning pods to nodes. Sounds EZ right? not that simple&mldr; (also read Julia Evans <a href=https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/>post</a> about the scheduler and the <a href=https://github.com/kubernetes/kubernetes/blob/989b2fd3715d01a7757e891de2a17de5a5c2cc91/pkg/scheduler/scheduler.go>github</a>)</p><p>The scheduler is a kind of a controller. The desired state is that every pod has a node assigned to it, it looks for pods without nodes, and tries to make actions towards the desired state (assigning them to nodes).</p><p>We could image the control loop like so</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>    pods <span style=color:#f92672>=</span> get_all_pods()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pod <span style=color:#f92672>in</span> pods:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> pod<span style=color:#f92672>.</span>node <span style=color:#f92672>==</span> nil:
</span></span><span style=display:flex><span>            assignNode(pod)
</span></span></code></pre></div><p>It&rsquo;s not exactly like that though&mldr;</p><p>After a little bit of digging, what actually happens inside the scheduler is:</p><ol><li>every pod that needs scheduling get added to a queue (the only resources scheduled are pods)</li><li>when a new pod is created it also gets added to the queue</li><li>the scheduler takes pods of the queue, and schedules them</li></ol><p>But what happens if a pods fails? If there is an error when scheduling the pod, it calls an error handler, and the error handler puts it back in the queue.</p><p>Ok wait&mldr; isn&rsquo;t the previous Python implementation better? for performance reasons, no&mldr;<br>For more info about scheduling optimizations read at the following: <a href=https://coreos.com/blog/improving-kubernetes-scheduler-performance.html>CoreOS - Improving Kubernetes Scheduler Performance</a></p><p><strong><em>Fun Fact</em></strong>: The scheduler uses an Informer too (like controllers usually do)</p><p><strong><em>Another Fun Fact</em></strong>: The scheduler unlike controllers, doesn&rsquo;t resync. and that&rsquo;s something that was decided by the maintainers:</p><blockquote><p>@brendandburns - what is it supposed to fix? I’m really against having such small resync periods, because it will significantly affect performance.</p></blockquote><p>and</p><blockquote><p>I agree with @wojtek-t . If resync ever fixes a problem, it means there is an underlying correctness bug that we are hiding. I do not think resync is the right solution.</p></blockquote><p>To sum up, the overall architecture looks roughly like so:</p><p><img alt=Image4 loading=lazy src=/img/kubernetes-control-plane-1.png></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://omer-do.github.io/tags/kubernetes/>Kubernetes</a></li><li><a href=https://omer-do.github.io/tags/containers/>Containers</a></li><li><a href=https://omer-do.github.io/tags/infrastructure/>Infrastructure</a></li></ul><nav class=paginav><a class=prev href=https://omer-do.github.io/posts/2020-12-16-how-it-all-fits-together/><span class=title>« Prev</span><br><span>Kubernetes - How It Fits Together (K8s series - 6)</span>
</a><a class=next href=https://omer-do.github.io/posts/2020-12-14-low-and-high-level-runtimes/><span class=title>Next »</span><br><span>Low and High Level Container Runtimes (K8s series - 4)</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://omer-do.github.io/>od-stack</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>